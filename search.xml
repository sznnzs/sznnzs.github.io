<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>bert-adapter初步分析</title>
    <url>/2021/09/03/adapter-bert_v0/</url>
    <content><![CDATA[<p>从bert与bert-adapter异同分析入手</p>
<a id="more"></a>

<p>tokenization.py中二者代码完全一致，没有任何区别（逐字符相同）</p>
<hr>
<p>run_classifier.py二者有细微区别</p>
<ol>
<li><p>bert中的<code>class XnliProcessor(DataProcessor):</code>在adapter-bert中被删除</p>
</li>
<li><p>bert的第593，596行中的output_weights，output_bias</p>
<p>其中的tf.get_variable函数添加了参数<code>collections=[&quot;head&quot;, tf.GraphKeys.GLOBAL_VARIABLES]</code></p>
</li>
<li><p>bert的787行<code>&quot;xnli&quot;: XnliProcessor,</code>在adapter-bert中被删除</p>
</li>
</ol>
<hr>
<p>optimization.py中二者有较小区别，主要为设置需要更新的参数仅仅为adapter层</p>
<ol>
<li><p>在AdamWeightDecayOptimizer中添加了<code>adapter_weight_decay_rate=0.01</code>的参数</p>
<p>adapter-bert中的62，96，107行</p>
</li>
<li><p>bert的70行定义了整个模型需要被训练的参数原先为 <code>tvars = tf.trainable_variables()</code></p>
<p>在adapter-bert中改成了：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tvars = []</span><br><span class="line"><span class="keyword">for</span> collection <span class="keyword">in</span> [<span class="string">&quot;adapters&quot;</span>, <span class="string">&quot;layer_norm&quot;</span>, <span class="string">&quot;head&quot;</span>]:</span><br><span class="line">  tvars += tf.get_collection(collection)</span><br></pre></td></tr></table></figure>
</li>
<li><p>adapter-bert中112行添加了</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">self._adapter_variable_names = &#123;</span><br><span class="line">    self._get_variable_name(v.name) <span class="keyword">for</span> v <span class="keyword">in</span> tf.get_collection(<span class="string">&quot;adapters&quot;</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>adapter-bert中155行，原先的bert为</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> self._do_use_weight_decay(param_name):</span><br><span class="line">  update += self.weight_decay_rate * param</span><br></pre></td></tr></table></figure>

<p>adapter-bert中为</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> self._do_use_weight_decay(param_name):</span><br><span class="line">  <span class="keyword">if</span> param_name <span class="keyword">in</span> self._adapter_variable_names:</span><br><span class="line">    update += self.adapter_weight_decay_rate * param</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    update += self.weight_decay_rate * param</span><br></pre></td></tr></table></figure>
</li>
<li><p>最后一处不同就是定义<code>def _do_use_weight_decay(self, param_name):</code>这个函数了，用来判断参数是否需要更新：</p>
<p>bert中为</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_do_use_weight_decay</span>(<span class="params">self, param_name</span>):</span></span><br><span class="line">  <span class="string">&quot;&quot;&quot;Whether to use L2 weight decay for `param_name`.&quot;&quot;&quot;</span></span><br><span class="line">  <span class="keyword">if</span> <span class="keyword">not</span> self.weight_decay_rate:</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">  <span class="keyword">if</span> self.exclude_from_weight_decay:</span><br><span class="line">    <span class="keyword">for</span> r <span class="keyword">in</span> self.exclude_from_weight_decay:</span><br><span class="line">      <span class="keyword">if</span> re.search(r, param_name) <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">  <span class="keyword">return</span> <span class="literal">True</span></span><br></pre></td></tr></table></figure>

<p>而在adapter-bert中为</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_do_use_weight_decay</span>(<span class="params">self, param_name</span>):</span></span><br><span class="line">  <span class="string">&quot;&quot;&quot;Whether to use L2 weight decay for `param_name`.&quot;&quot;&quot;</span></span><br><span class="line">  <span class="keyword">if</span> param_name <span class="keyword">in</span> self._adapter_variable_names:</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> self.adapter_weight_decay_rate:</span><br><span class="line">      <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> self.weight_decay_rate:</span><br><span class="line">      <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">   </span><br><span class="line">  <span class="keyword">if</span> self.exclude_from_weight_decay:</span><br><span class="line">    <span class="keyword">for</span> r <span class="keyword">in</span> self.exclude_from_weight_decay:</span><br><span class="line">      <span class="keyword">if</span> re.search(r, param_name) <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">   </span><br><span class="line">  <span class="keyword">return</span> <span class="literal">True</span></span><br></pre></td></tr></table></figure>

</li>
</ol>
<hr>
<p>最重要的modeling.py，下面行数以adapter-bert为基准</p>
<ol>
<li><p>139行的init函数添加参数<code>adapter_fn=&quot;feedforward_adapter&quot;</code></p>
<p>参数解释：adapter_fn: (optional) string identifying trainable adapter that takes as input a Tensor and returns a Tensor of the same shape.</p>
<p>主要就是经过adapter模块之后向量的形状不发生改变</p>
<p>208行<code>self.all_encoder_layers = transformer_model(...)</code>中添加参数：<code>adapter_fn=get_adapter(adapter_fn)</code></p>
</li>
<li><p>代码比对软件中最右侧状态栏蓝色部分的意思是新添加上去的（而黑色栏是这部分发生了修改）</p>
<p>两个modeling.py一对比，发现有很多蓝色的部分，而黑色的修改都很微弱</p>
<p>adapter-bert中新增的函数，也是adapter方法的核心（从321行开始）：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">feedforward_adapter</span>(<span class="params">input_tensor, hidden_size=<span class="number">64</span>, init_scale=<span class="number">1e-3</span></span>):</span></span><br><span class="line">  <span class="string">&quot;&quot;&quot;A feedforward adapter layer with a bottleneck.</span></span><br><span class="line"><span class="string">  Implements a bottleneck layer with a user-specified nonlinearity and an</span></span><br><span class="line"><span class="string">  identity residual connection. All variables created are added to the</span></span><br><span class="line"><span class="string">  &quot;adapters&quot; collection.</span></span><br><span class="line"><span class="string">  Args:</span></span><br><span class="line"><span class="string">    input_tensor: input Tensor of shape [batch size, hidden dimension]</span></span><br><span class="line"><span class="string">    hidden_size: dimension of the bottleneck layer.</span></span><br><span class="line"><span class="string">    init_scale: Scale of the initialization distribution used for weights.</span></span><br><span class="line"><span class="string">  Returns:</span></span><br><span class="line"><span class="string">    Tensor of the same shape as x.</span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br><span class="line">  <span class="keyword">with</span> tf.variable_scope(<span class="string">&quot;adapters&quot;</span>):</span><br><span class="line">    in_size = input_tensor.get_shape().as_list()[<span class="number">1</span>]</span><br><span class="line">    w1 = tf.get_variable(</span><br><span class="line">        <span class="string">&quot;weights1&quot;</span>, [in_size, hidden_size],</span><br><span class="line">        initializer=tf.truncated_normal_initializer(stddev=init_scale),</span><br><span class="line">        collections=[<span class="string">&quot;adapters&quot;</span>, tf.GraphKeys.GLOBAL_VARIABLES])</span><br><span class="line">    b1 = tf.get_variable(</span><br><span class="line">        <span class="string">&quot;biases1&quot;</span>, [<span class="number">1</span>, hidden_size],</span><br><span class="line">        initializer=tf.zeros_initializer(),</span><br><span class="line">        collections=[<span class="string">&quot;adapters&quot;</span>, tf.GraphKeys.GLOBAL_VARIABLES])</span><br><span class="line">    net = tf.tensordot(input_tensor, w1, [[<span class="number">1</span>], [<span class="number">0</span>]]) + b1</span><br><span class="line"></span><br><span class="line">    net = gelu(net)</span><br><span class="line"></span><br><span class="line">    w2 = tf.get_variable(</span><br><span class="line">        <span class="string">&quot;weights2&quot;</span>, [hidden_size, in_size],</span><br><span class="line">        initializer=tf.truncated_normal_initializer(stddev=init_scale),</span><br><span class="line">        collections=[<span class="string">&quot;adapters&quot;</span>, tf.GraphKeys.GLOBAL_VARIABLES])</span><br><span class="line">    b2 = tf.get_variable(</span><br><span class="line">        <span class="string">&quot;biases2&quot;</span>, [<span class="number">1</span>, in_size],</span><br><span class="line">        initializer=tf.zeros_initializer(),</span><br><span class="line">        collections=[<span class="string">&quot;adapters&quot;</span>, tf.GraphKeys.GLOBAL_VARIABLES])</span><br><span class="line">    net = tf.tensordot(net, w2, [[<span class="number">1</span>], [<span class="number">0</span>]]) + b2</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> net + input_tensor</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_adapter</span>(<span class="params">function_string</span>):</span></span><br><span class="line">  <span class="string">&quot;&quot;&quot;Maps a string to a Python function.</span></span><br><span class="line"><span class="string">  Args:</span></span><br><span class="line"><span class="string">    function_string: String name of the adapter function.</span></span><br><span class="line"><span class="string">  Returns:</span></span><br><span class="line"><span class="string">    A Python function corresponding to the adatper function.</span></span><br><span class="line"><span class="string">    `function_string` is None or empty, will return None.</span></span><br><span class="line"><span class="string">    If `function_string` is not a string, it will return `function_string`.</span></span><br><span class="line"><span class="string">  Raises:</span></span><br><span class="line"><span class="string">    ValueError: The `function_string` does not correspond to a known</span></span><br><span class="line"><span class="string">      adapter.</span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># We assume that anything that&quot;s not a string is already an adapter</span></span><br><span class="line">  <span class="comment"># function, so we just return it.</span></span><br><span class="line">  <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(function_string, six.string_types):</span><br><span class="line">    <span class="keyword">return</span> function_string</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> <span class="keyword">not</span> function_string:</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">  fn = function_string.lower()</span><br><span class="line">  <span class="keyword">if</span> fn == <span class="string">&quot;feedforward_adapter&quot;</span>:</span><br><span class="line">    <span class="keyword">return</span> feedforward_adapter</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    <span class="keyword">raise</span> ValueError(<span class="string">&quot;Unsupported adapters: %s&quot;</span> % fn)</span><br></pre></td></tr></table></figure>

<p>注意把adapter的参数加进了全局的collection，adapter的代码实现非常简单</p>
</li>
<li><p>439行layer_norm的修改：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">layer_norm</span>(<span class="params">input_tensor, name=<span class="literal">None</span></span>):</span></span><br><span class="line">  <span class="string">&quot;&quot;&quot;Run layer normalization on the last dimension of the tensor.&quot;&quot;&quot;</span></span><br><span class="line">  <span class="keyword">return</span> tf.contrib.layers.layer_norm(</span><br><span class="line">      inputs=input_tensor, begin_norm_axis=-<span class="number">1</span>, begin_params_axis=-<span class="number">1</span>, scope=name,</span><br><span class="line">      variables_collections=[<span class="string">&quot;layer_norm&quot;</span>, tf.GraphKeys.GLOBAL_VARIABLES])</span><br></pre></td></tr></table></figure>

<p>新添加了最后一行的参数</p>
</li>
<li><p>代码843行中<code>def transformer_model(...)</code>添加了参数<code>adapter_fn=None</code></p>
<p>adapter流程在两处添加：938行的attention_output和964行的layer_output：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Run a linear projection of `hidden_size` then add a residual</span></span><br><span class="line"><span class="comment"># with `layer_input`.</span></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">&quot;output&quot;</span>):</span><br><span class="line">  attention_output = tf.layers.dense(</span><br><span class="line">      attention_output,</span><br><span class="line">      hidden_size,</span><br><span class="line">      kernel_initializer=create_initializer(initializer_range))</span><br><span class="line">  attention_output = dropout(attention_output, hidden_dropout_prob)</span><br><span class="line">  <span class="keyword">if</span> adapter_fn:</span><br><span class="line">    attention_output = adapter_fn(attention_output)</span><br><span class="line">  attention_output = layer_norm(attention_output + layer_input)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Down-project back to `hidden_size` then add the residual.</span></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">&quot;output&quot;</span>):</span><br><span class="line">  layer_output = tf.layers.dense(</span><br><span class="line">      intermediate_output,</span><br><span class="line">      hidden_size,</span><br><span class="line">      kernel_initializer=create_initializer(initializer_range))</span><br><span class="line">  layer_output = dropout(layer_output, hidden_dropout_prob)</span><br><span class="line">  <span class="keyword">if</span> adapter_fn:</span><br><span class="line">    layer_output = adapter_fn(layer_output)</span><br><span class="line">  layer_output = layer_norm(layer_output + attention_output)</span><br><span class="line">  prev_output = layer_output</span><br><span class="line">  all_layer_outputs.append(layer_output)</span><br></pre></td></tr></table></figure>

</li>
</ol>
]]></content>
      <categories>
        <category>nlp_bert</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>adapter</tag>
      </tags>
  </entry>
</search>
